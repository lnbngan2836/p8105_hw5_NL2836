p8105_hw5_NL2836
================
Ngan Le
2023-11-15

# Question 1

##### Import dataset from CSV file.

``` r
homicide <- read_csv("homicide-data.csv")
```

##### Describe the raw data.

This dataset includes 52179 observations and 12 variables, reporting the
date, the city, the state, the coordinates of the incidences, the
victims’ name, age, race, gender, and the disposition of the case. The
incidences are reported across 28 states from 2007-2017.

##### Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

``` r
homicide =
  homicide %>%
  mutate(city_state = str_c(city, ", ", state)) 
 
city_summary =
  homicide %>% 
  group_by(city_state) %>% 
  summarize(
    city_total= n(), 
    city_unsolved= sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

- From 2007-2017, Chicago had the highest number of total number of
  homicides and also the highest number of unsolved homicides. Tulsa, AL
  only had 1 reported case and 0 reported unsolved case, which might
  have been due to missing reports. If we exclude Tulsa, AL then Tampa,
  FL had both the lowest number of total homicides and the lowest number
  of unsolved homicides.

##### For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

``` r
baltimore =
  city_summary %>% 
  filter(city_state == "Baltimore, MD")

test =
  prop.test(
  x= baltimore %>% pull(city_unsolved),
  n = baltimore %>%pull(city_total))%>% 
  broom::tidy()

test
```

    ## # A tibble: 1 × 8
    ##   estimate statistic  p.value parameter conf.low conf.high method    alternative
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr>     <chr>      
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sample… two.sided

- The estimated proportion of homicides in Baltimore, MD is 64.55% (95%
  CI: 62.8% - 66.3%).

##### Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

``` r
city_props = 
  city_summary %>% 
  mutate(
    test = map2(.x = city_unsolved, .y = city_total, ~prop.test(x = .x, n = .y)),
    test = map(test,broom::tidy))%>%
  unnest() %>%
  select(city_state, estimate, conf.low, conf.high)%>%
  arrange(desc(estimate))

city_props
```

    ## # A tibble: 51 × 4
    ##    city_state         estimate conf.low conf.high
    ##    <chr>                 <dbl>    <dbl>     <dbl>
    ##  1 Chicago, IL           0.736    0.724     0.747
    ##  2 New Orleans, LA       0.649    0.623     0.673
    ##  3 Baltimore, MD         0.646    0.628     0.663
    ##  4 San Bernardino, CA    0.618    0.558     0.675
    ##  5 Buffalo, NY           0.612    0.569     0.654
    ##  6 Miami, FL             0.605    0.569     0.640
    ##  7 Stockton, CA          0.599    0.552     0.645
    ##  8 Detroit, MI           0.588    0.569     0.608
    ##  9 Phoenix, AZ           0.551    0.518     0.584
    ## 10 Denver, CO            0.542    0.485     0.598
    ## # ℹ 41 more rows

- Chicago, IL had the highest percentage of unsolved homicides during
  2007-2017, which is at 73.6% (95% CI: 72.4% - 74.7%). Excluding Tulsa,
  AL for the same reason mentioned above, Richmond, VA had the lowest
  percentage of unsolved homocides during 2007-2017, which is at 26.3%
  (95% CI: 22.3% - 30.8%)

##### Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

Here we will exclude Tulsa, AL from the dataset to keep the plot clear
and tidy.

``` r
city_props =
  city_props %>%
  filter(city_state != "Tulsa, AL")

ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.7) +
  coord_flip() +
  labs(x = "City, State", y = "Proportion of Unsolved Homicides and 95% CI", 
       title = "Proportion of Unsolved Homicides in Each City and Confidence Intervals") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))
```

![](p8105_hw5_NL2836_files/figure-gfm/city_unsolved%20plots-1.png)<!-- -->

- Chicago, IL’s percentage of unsolved homicides seems to be an outlier
  (by visual assessment) among the cities reported.

# Question 2

Import dataset.

``` r
df = 
  tibble(list.files("./data")) %>%
  mutate(file_list = paste(list.files("./data/")))

read_files = function(x) {
  
    data = read_csv(paste0("./data/", x)) %>%
      mutate(file_names = x)
}

longit_raw <- map_df(df$file_list, read_files)
```

Tidy dataset.

``` r
longit_tidy =
  longit_raw %>%
  janitor::clean_names() %>%
  mutate(
    file_names = str_replace(file_names, ".csv", ""),
    group = str_sub(file_names, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "obs",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  mutate(subject = as.integer(str_extract(file_names, "[0-9][0-9]"))) %>%
  select(group, subject, week, obs)

longit_tidy
```

    ## # A tibble: 160 × 4
    ##    group subject  week   obs
    ##    <chr>   <int> <dbl> <dbl>
    ##  1 con         1     1  0.2 
    ##  2 con         1     2 -1.31
    ##  3 con         1     3  0.66
    ##  4 con         1     4  1.96
    ##  5 con         1     5  0.23
    ##  6 con         1     6  1.09
    ##  7 con         1     7  0.05
    ##  8 con         1     8  1.94
    ##  9 con         2     1  1.13
    ## 10 con         2     2 -0.88
    ## # ℹ 150 more rows

``` r
wide_tidy <- longit_tidy %>%
  pivot_wider(
    names_from = week,  
    values_from = obs,  
    names_prefix = "week_"  
  )

wide_tidy
```

    ## # A tibble: 20 × 10
    ##    group subject week_1 week_2 week_3 week_4 week_5 week_6 week_7 week_8
    ##    <chr>   <int>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
    ##  1 con         1   0.2   -1.31   0.66   1.96   0.23   1.09   0.05   1.94
    ##  2 con         2   1.13  -0.88   1.07   0.17  -0.83  -0.31   1.58   0.44
    ##  3 con         3   1.77   3.11   2.22   3.26   3.31   0.89   1.88   1.01
    ##  4 con         4   1.04   3.66   1.22   2.33   1.47   2.7    1.87   1.66
    ##  5 con         5   0.47  -0.58  -0.09  -1.37  -0.32  -2.17   0.45   0.48
    ##  6 con         6   2.37   2.5    1.59  -0.16   2.08   3.07   0.78   2.35
    ##  7 con         7   0.03   1.21   1.13   0.64   0.49  -0.12  -0.07   0.46
    ##  8 con         8  -0.08   1.42   0.09   0.36   1.18  -1.16   0.33  -0.44
    ##  9 con         9   0.08   1.24   1.44   0.41   0.95   2.75   0.3    0.03
    ## 10 con        10   2.14   1.15   2.52   3.44   4.26   0.97   2.73  -0.53
    ## 11 exp         1   3.05   3.67   4.84   5.8    6.33   5.46   6.38   5.91
    ## 12 exp         2  -0.84   2.63   1.64   2.58   1.24   2.32   3.11   3.78
    ## 13 exp         3   2.15   2.08   1.82   2.84   3.36   3.61   3.37   3.74
    ## 14 exp         4  -0.62   2.54   3.78   2.73   4.49   5.82   6      6.49
    ## 15 exp         5   0.7    3.33   5.34   5.57   6.9    6.66   6.24   6.95
    ## 16 exp         6   3.73   4.08   5.4    6.41   4.87   6.09   7.66   5.83
    ## 17 exp         7   1.18   2.35   1.23   1.17   2.02   1.61   3.13   4.88
    ## 18 exp         8   1.37   1.43   1.84   3.6    3.8    4.72   4.68   5.7 
    ## 19 exp         9  -0.4    1.08   2.66   2.7    2.8    2.64   3.51   3.27
    ## 20 exp        10   1.09   2.8    2.8    4.3    2.25   6.57   6.09   4.64

###### Make a plot

``` r
longit_tidy %>% 
  ggplot(aes(x = week, y = obs, color = as.factor(subject))) +
  geom_point(size=0.2) +
  geom_line(aes(group = subject), alpha=0.5) +
  facet_grid(~group) +
  labs(x = "Week", y = "Observation", col = "Subject ID")
```

![](p8105_hw5_NL2836_files/figure-gfm/spaghetti%20plot-1.png)<!-- -->

- Among the control group, the observation values stay around 0 - 2.5
  (units of measurement) and relatively constant across 8 weeks.
- Among the exposed group, the observation values have a higher initial
  value and across 8 weeks witness an upward trend.

# Question 3
