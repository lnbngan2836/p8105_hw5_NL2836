---
title: "p8105_hw5_NL2836"
author: "Ngan Le"
date: "2023-11-15"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readr)
library(rvest)
library(dplyr)
library(patchwork)
```

# Question 1

##### Import dataset from CSV file. 

```{r import homicide data, message = FALSE}
homicide <- read_csv("homicide-data.csv")
```

##### Describe the raw data. 

This dataset includes `r nrow(homicide)` observations and `r ncol(homicide)` variables, reporting the date, the city, the state, the coordinates of the incidences, the victims' name, age, race, gender, and the disposition of the case. The incidences are reported across `r n_distinct(homicide$state)` states from 2007-2017. 

#####  Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r city summary}
homicide =
  homicide %>%
  mutate(city_state = str_c(city, ", ", state)) 
 
city_summary =
  homicide %>% 
  group_by(city_state) %>% 
  summarize(
    city_total= n(), 
    city_unsolved= sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )
```

- From 2007-2017, Chicago had the highest number of total number of homicides and also the highest number of unsolved homicides. Tulsa, AL only had 1 reported case and 0 reported unsolved case, which might have been due to missing reports. If we exclude Tulsa, AL then Tampa, FL had both the lowest number of total homicides and the lowest number of unsolved homicides. 

##### For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r baltimore props}
baltimore =
  city_summary %>% 
  filter(city_state == "Baltimore, MD")

test =
  prop.test(
  x= baltimore %>% pull(city_unsolved),
  n = baltimore %>%pull(city_total))%>% 
  broom::tidy()

test
```
- The estimated proportion of homicides in Baltimore, MD is 64.55% (95% CI: 62.8% -  66.3%).


##### Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

```{r city_unsolved props, warning = FALSE}
city_props = 
  city_summary %>% 
  mutate(
    test = map2(.x = city_unsolved, .y = city_total, ~prop.test(x = .x, n = .y)),
    test = map(test,broom::tidy))%>%
  unnest() %>%
  select(city_state, estimate, conf.low, conf.high)%>%
  arrange(desc(estimate))

city_props
```
- Chicago, IL had the highest percentage of unsolved homicides during 2007-2017, which is at 73.6% (95% CI: 72.4% - 74.7%). Excluding Tulsa, AL for the same reason mentioned above, Richmond, VA had the lowest percentage of unsolved homocides during 2007-2017, which is at 26.3% (95% CI: 22.3% - 30.8%)

##### Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

Here we will exclude Tulsa, AL from the dataset to keep the plot clear and tidy.

```{r city_unsolved plots, dpi = 500}
city_props =
  city_props %>%
  filter(city_state != "Tulsa, AL")

ggplot(city_props, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.7) +
  coord_flip() +
  labs(x = "City, State", y = "Proportion of Unsolved Homicides and 95% CI", 
       title = "Proportion of Unsolved Homicides in Each City and Confidence Intervals") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))
```

- Chicago, IL's percentage of unsolved homicides seems to be an outlier (by visual assessment) among the cities reported. 

# Question 2

Import dataset.

```{r import longit data, message = FALSE}
df = 
  tibble(list.files("./data")) %>%
  mutate(file_list = paste(list.files("./data/")))

read_files = function(x) {
  
    data = read_csv(paste0("./data/", x)) %>%
      mutate(file_names = x)
}

longit_raw <- map_df(df$file_list, read_files)
```

Tidy dataset.

```{r}
longit_tidy =
  longit_raw %>%
  janitor::clean_names() %>%
  mutate(
    file_names = str_replace(file_names, ".csv", ""),
    group = str_sub(file_names, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "obs",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  mutate(subject = as.integer(str_extract(file_names, "[0-9][0-9]"))) %>%
  select(group, subject, week, obs)

longit_tidy
```

```{r}
wide_tidy <- longit_tidy %>%
  pivot_wider(
    names_from = week,  
    values_from = obs,  
    names_prefix = "week_"  
  )

wide_tidy
```

###### Make a plot

```{r spaghetti plot, dpi = 300}
longit_tidy %>% 
  ggplot(aes(x = week, y = obs, color = as.factor(subject))) +
  geom_point(size=0.2) +
  geom_line(aes(group = subject), alpha=0.5) +
  facet_grid(~group) +
  labs(x = "Week", y = "Observation", col = "Subject ID")
```

- Among the control group, the observation values stay around 0 - 2.5 (units of measurement) and relatively constant across 8 weeks. 
- Among the exposed group, the observation values have a higher initial value and across 8 weeks witness an upward trend. 

# Question 3

##### First set the following design elements: Fix n=30, Fix σ=5, Set μ=0. Generate 5000 datasets from the model.

```{r}
set.seed(123456) 
n = 30
sigma = 5
mu_values = 0:6
num_simulations = 5000
alpha = 0.05

simulation_results = 
  map_dfr(mu_values, function(mu) {
  tibble(
    mu = mu,
    simulation = 1:num_simulations
  ) %>%
  mutate(
    data = map(simulation, ~ rnorm(n, mu, sigma)),
    t_test = map(data, ~ broom::tidy(t.test(.x, mu = 0))),
    p_value = map_dbl(t_test, ~ .x$p.value),
    mu_hat = map_dbl(t_test, ~ .x$estimate),
    reject_null = p_value < alpha
  )
})
```

```{r}
power_plot =
  simulation_results %>%
  group_by(mu) %>%
  summarize(power = mean(reject_null)) %>%
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  labs(title = "Power vs True Mean",
       x = "True mean", y = "Power")

mu_hat_plot =
  simulation_results %>%
  group_by(mu) %>%
  summarize(avg_mu_hat = mean(mu_hat), 
            avg_mu_hat_rejected = mean(mu_hat[reject_null])) %>%
  ggplot() +
  geom_line(aes(x = mu, y = avg_mu_hat), color = "blue") +
  geom_line(aes(x = mu, y = avg_mu_hat_rejected), color = "red") +
  labs(title = "Average Estimate of Mean versus True Mean",
       x = "True Mean", y = "Average Estimate of Mean")
```

```{r}
power_plot
```

- This plot shows how the power of the test (probability of rejecting a false null hypothesis) changes as the value of true mean increases. As the effect size (difference from the null hypothesis, which states that true means equals 0) increases, the power also increases.

```{r}
mu_hat_plot
```

- This plot shows the average estimated mu_hat across all tests (in blue) and for tests where the null was rejected (in red). If the test is unbiased, the sample average of mu_hat for all tests and rejected should be approximately equal.

